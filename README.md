# Sparkify-Data-Pipelines

This repository showcases the capstone project of my Udacity Data Engineering Nanodegree, where I designed and deployed scalable, automated data pipelines for a fictional music streaming company, Sparkify. The project leverages Apache Airflow, AWS Redshift, and S3 to simulate a real-world ETL workflow for transforming and loading raw JSON log data into an optimized data warehouse schema for analytical use.

## Project Overview

Sparkify, a fast-growing music streaming startup, is enhancing its data infrastructure by introducing automation and monitoring into its data warehouse ETL pipelines. To achieve this, the company has chosen Apache Airflow as the orchestration tool for building robust, scalable, and modular data pipelines.

The goal is to design pipelines that are:
- Dynamic and reusable across various workflows
- Monitored and scheduled for reliability
- Backfill-capable to recover from missed data loads
- Testable for ensuring data quality after ETL transformations

The raw data, stored in Amazon S3, includes:
- JSON event logs capturing user activity within the application
- JSON song metadata related to the tracks users listen to

These datasets are extracted, transformed, and loaded into a star-schema structure within Amazon Redshift, enabling efficient querying and analysis.

As part of this project, I implemented the core concepts of Apache Airflow by:
- Developing custom Airflow operators for staging data, loading fact and dimension tables, and performing data quality checks
- Assembling and linking modular tasks to form a coherent and functional DAG (Directed Acyclic Graph)
- Utilized the provided project template to build custom Airflow operators and adapt reusable SQL helper functions to execute ETL logic

This project emphasizes hands-on workflow orchestration, focusing on automation, data validation, and pipeline design in a real-world data engineering context.

## Project Datasets

The Project has two datasets that reside in AWS S3 Bucket:
- Log data: s3://udacity-dend/log_data
- Song data: s3://udacity-dend/song-data

**Log Dataset**: This dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The files are partitioned by year and month.

**Song Dataset**: This dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

## Project Environment

This project was developed and executed in a cloud-based environment using the following technologies:

**Apache Airflow**: used as the orchestration tool to design, schedule, and monitor automated ETL pipelines. Custom operators and DAGs were created to manage data flow and quality checks.

**Amazon Redshift**: used as the data warehouse for storing transformed data in a star-schema format, optimized for analytical queries.

**Amazon S3**: served as the data lake for storing raw JSON log files and song metadata. Airflow pipelines pulled data from S3 for processing.

**Python**: custom Airflow operators and supporting scripts were written in Python to enable dynamic data processing and logic control.

**SQL**: SQL queries were used for staging, transforming, and validating data within Redshift. A helper module provided reusable SQL logic.

This environment simulates a real-world data engineering setup, integrating cloud storage, data warehousing, and workflow orchestration to ensure reliable and scalable data pipelines.

## Implementation

<p align="center">
  <img src="assets/Successful Run.png" alt="Airflow DAG Successful Run" width="700"/>
  <br>
  <em>Successful DAG run in Apache Airflow for Sparkify ETL pipeline.</em>
</p>

The project was implemented in a series of structured steps to simulate a production-grade data pipeline that meets the analytical needs of Sparkifyâ€™s data team:

1. **Data Modeling** 
    - Designed a star schema with one fact and multiple dimension tables to support analytical queries related to user activity and song metadata.


2. **AWS Configuration**
    - Created an IAM user with appropriate permissions to access S3 and Redshift.
    - Set up an Amazon Redshift cluster, configured security groups, and connected it with S3 for data ingestion.

3. **Airflow Integration**
    - Established a connection between Apache Airflow and AWS using credentials.
    - Configured Airflow connections to interact with the Redshift cluster securely.

4. **DAG Configuration**
    - Defined the DAG structure with default parameters such as retries, start date, and schedule interval.
    - Established task dependencies to ensure the proper flow of data through the pipeline.

5. **Customer Operator Development**

    To modularize the ETL pipeline and enable reusability, I developed four custom Airflow operators, each responsible for a specific stage in the data pipeline:
    - **StageToRedshiftOperator**
        - Handles staging of raw JSON data from Amazon S3 to Amazon Redshift.
        - Constructs and executes a parameterized COPY command to load data from S3 into staging tables in Redshift.
        - Differentiates between log and song datasets by using parameters to specify the target table and JSON path file.

    - **LoadFactOperator**
        - Responsible for loading data into the fact table (songplays) from staging tables.
        - Executes a provided SQL transformation query to populate the fact table.
        - Uses append-only logic, appropriate for large fact tables that continuously grow.

    - **LoadDimensionOperator**
        - Loads data into the four dimension tables: users, songs, artists, and time.
        - Accepts SQL queries and the target table as parameters.
        - uses append-only logic, for cases where data should not be deleted.

    - **DataQualityOperator**
        - Performs data validation by executing one or more SQL test cases and comparing the actual results to expected outcomes.
        - Designed to raise an exception if any test fails, allowing Airflow's retry and failure logic to kick in.
        - Example: a test may check for Zero records in a table, the pipeline fails if there are no records.

6. **Pipeline Execution**
    - Triggered the DAG to perform the full ETL process: staging raw JSON data, transforming it, and loading it into the Redshift warehouse.

    <p align="center">
    <img src="assets/Successful Run Details.png" alt="Airflow DAG Successful Run" width="700"/>
    <br>
    <em>Successful DAG run details.</em>
    </p>

7. **Data Validation**
    - Implemented data quality checks to ensure tables were populated correctly and contained no null or unexpected values.

This pipeline design enables automation, monitoring, and scalability, providing Sparkify with a robust solution for managing and analyzing large volumes of streaming data.
